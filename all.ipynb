{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "#import sys, time, datetime, random, math, pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# auc and roc\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "# Balanced accuracy\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Classification report\n",
    "from sklearn.metrics import recall_score, balanced_accuracy_score, f1_score, roc_auc_score\n",
    "# SVG\n",
    "#from IPython.display import SVG, display\n",
    "# Save sklearn model\n",
    "import joblib\n",
    "#import warnings\n",
    "import xgboost as xgb\n",
    "#import gc\n",
    "import torchviz\n",
    "\n",
    "\n",
    "import lookup_dicts\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning based Automatic Quality Control of Climatological Greenlantic Sensor data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionaries\n",
    "ELEM_DICT = lookup_dicts.ELEM_DICT\n",
    "INDEX_TO_FEATURE = lookup_dicts.INDEX_TO_FEATURE\n",
    "PATH_DATA = lookup_dicts.PATH_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "X_train = torch.load(PATH_DATA['train']['X']).float()\n",
    "y_train = torch.load(PATH_DATA['train']['y']).float().squeeze()\n",
    "# Load validation data\n",
    "X_val = torch.load(PATH_DATA['val']['X']).float()\n",
    "y_val = torch.load(PATH_DATA['val']['y']).float().squeeze()\n",
    "# Load test data\n",
    "X_test = torch.load(PATH_DATA['test']['X']).float()\n",
    "y_test = torch.load(PATH_DATA['test']['y']).float().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 1958 to 2011, and X_val from 2011 to 2017, and X_test from 2017 to 2022.\n",
      "X_train.shape = torch.Size([41176872, 7]).\n",
      "num_features = 7.\n",
      "num_classes = 1.\n",
      "output_size := 1.\n",
      "num_workers := 12.\n",
      "DEVICE := cuda.\n"
     ]
    }
   ],
   "source": [
    "# Assert that X_train[0,3] <= X_train[-1,3] <= X_val[0,3] <= X_val[-1,3] <= X_test[0,3] <= X_test[-1,3]\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = y_train.shape[1] if len(y_train.shape) > 1 else 1\n",
    "num_workers = os.cpu_count()\n",
    "output_size = num_classes\n",
    "\n",
    "assert (\n",
    "    X_train[0, 3] <= X_train[-1, 3]\n",
    "    <=  X_val[0, 3] <= X_val[ -1,3]\n",
    "    <= X_test[0, 3] <= X_test[-1,3]), \"Data is not sorted by time.\"\n",
    "print(f'X_train from {str(int(X_train[0,3]))} to {str(int(X_train[-1,3]))}, and X_val from {str(int(X_val[0,3]))} to {str(int(X_val[-1,3]))}, and X_test from {str(int(X_test[0,3]))} to {str(int(X_test[-1,3]))}.')\n",
    "print(f'X_train.shape = {X_train.shape}.')\n",
    "print(f'num_features = {num_features}.')\n",
    "print(f'num_classes = {num_classes}.')\n",
    "print(f'output_size := {output_size}.')\n",
    "print(f'num_workers := {num_workers}.')\n",
    "print(f'DEVICE := {DEVICE}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return (tp/(tp+fn) + tn/(tn+fp))/2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 models: 1. Logistic Regression, 2. XGBoost, 3. Multi-Layer Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make class that inherits from sklearn's LogisticRegression and overrides the fit method\n",
    "class LogisticRegression(LogisticRegression):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.class_weight = 'balanced'\n",
    "        self.max_iter = 1000\n",
    "        self.n_jobs = -1\n",
    "        self.verbose = 1\n",
    "    def fit(self, X, y, sample_weight=None, **kwargs):\n",
    "        # Save new model\n",
    "        print('Saving new model...')\n",
    "        self.X = X.to('cpu').numpy()\n",
    "        self.y = y.to('cpu').numpy().squeeze()\n",
    "        self.sample_weight = sample_weight.to('cpu').numpy().squeeze() if sample_weight is not None else None\n",
    "        logreg = super().fit(self.X, self.y, sample_weight=self.sample_weight)\n",
    "        PATH_TO_SAVE = 'models/checkpoint/logreg.pkl'\n",
    "        if not os.path.exists(os.path.dirname(PATH_TO_SAVE)):\n",
    "            os.makedirs(os.path.dirname(PATH_TO_SAVE))\n",
    "        # Use joblib to save the model\n",
    "        joblib_file = PATH_TO_SAVE\n",
    "        joblib.dump(logreg, joblib_file)\n",
    "        return logreg\n",
    "    def predict(self, X):\n",
    "        return super().predict(X)\n",
    "    def predict_proba(self, X):\n",
    "        return super().predict_proba(X)\n",
    "    def save_predictions(self, X, y, data_set='test'):\n",
    "        if type(X) == torch.Tensor:\n",
    "            X = X.to('cpu').numpy()\n",
    "        if type(y) == torch.Tensor:\n",
    "            y = y.to('cpu').numpy().squeeze()\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        y_pred_proba = y_pred_proba[:, 1]\n",
    "        PATH_TO_SAVE = f'data/{data_set}/preds/logreg/'\n",
    "        if not os.path.exists(os.path.dirname(PATH_TO_SAVE)):\n",
    "            os.makedirs(os.path.dirname(PATH_TO_SAVE))\n",
    "        np.save(PATH_TO_SAVE + 'y_pred.npy', y_pred)\n",
    "        np.save(PATH_TO_SAVE + 'y_pred_proba.npy', y_pred_proba)\n",
    "        np.save(PATH_TO_SAVE + 'y_true.npy', y)\n",
    "# Initialize the model\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dmatrices...\n",
      "Using weights: w_0_train = 63.51287366892007, w_1_train = 0.01574483946692131\n",
      "Using scale_pos_weight = 0.01526492833161295\n"
     ]
    }
   ],
   "source": [
    "# X_train, y_train, X_val, y_val, X_test, y_test in numpy format\n",
    "X_train_np, y_train_np, X_val_np, y_val_np, X_test_np, y_test_np = X_train.to('cpu').numpy(), y_train.to(\n",
    "    'cpu').numpy(), X_val.to('cpu').numpy(), y_val.to('cpu').numpy(), X_test.to('cpu').numpy(), y_test.to('cpu').numpy()\n",
    "\n",
    "\n",
    "def make_dmatrices_weighted(X_train, y_train, X_val, y_val, X_test, y_test, use_same_dmatrices=True):\n",
    "    # Check if dtrain, dval, dtest already exist\n",
    "    if (use_same_dmatrices and (os.path.exists('data/dmatrices/dtrain.buffer') and os.path.exists('data/dmatrices/dval.buffer') and os.path.exists('data/dmatrices/dtest.buffer'))):\n",
    "        print('Loading dmatrices...')\n",
    "        dtrain = xgb.DMatrix('data/dmatrices/dtrain.buffer', nthread=-1)\n",
    "        dval = xgb.DMatrix('data/dmatrices/dval.buffer', nthread=-1)\n",
    "        dtest = xgb.DMatrix('data/dmatrices/dtest.buffer', nthread=-1)\n",
    "        weights = dtrain.get_weight().squeeze()\n",
    "        # Get unique weights\n",
    "        weights = np.unique(weights)\n",
    "        print(f'Using weights: w_0_train = {weights[0]}, w_1_train = {weights[1]}')\n",
    "        return dtrain, dval, dtest\n",
    "    print('Creating dmatrices...')\n",
    "    N_0 = np.sum(y_train == 0)\n",
    "    N_1 = np.sum(y_train == 1)\n",
    "    w_0_train = (2*N_1) / N_0\n",
    "    w_1_train = N_0 / (N_1*2)\n",
    "    print(f'Using weights: w_0_train = {w_0_train}, w_1_train = {w_1_train}')\n",
    "    weights_train = np.zeros(len(y_train))\n",
    "    weights_train[y_train == 0] = w_0_train\n",
    "    weights_train[y_train == 1] = w_1_train\n",
    "    weights_val = np.zeros(len(y_val))\n",
    "    weights_val[y_val == 0] = w_0_train\n",
    "    weights_val[y_val == 1] = w_1_train\n",
    "    weights_test = np.zeros(len(y_test))\n",
    "    weights_test[y_test == 0] = w_0_train\n",
    "    weights_test[y_test == 1] = w_1_train\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, weight=weights_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val, weight=weights_val)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test, weight=weights_test)\n",
    "    # Check if folder exists for dmatrices\n",
    "    if not os.path.exists('data/dmatrices/'):\n",
    "        os.makedirs('data/dmatrices/')\n",
    "    # Save dmatrices\n",
    "    dtrain.save_binary('data/dmatrices/dtrain.buffer')\n",
    "    dval.save_binary('data/dmatrices/dval.buffer')\n",
    "    dtest.save_binary('data/dmatrices/dtest.buffer')\n",
    "    return dtrain, dval, dtest\n",
    "\n",
    "dtrain, dval, dtest = make_dmatrices_weighted(X_train_np, y_train_np, X_val_np, y_val_np, X_test_np, y_test_np, use_same_dmatrices=False)\n",
    "\n",
    "positives = np.sum(dtrain.get_label())  # The number of positive samples\n",
    "total = np.shape(dtrain.get_label())[0]  # The total number of samples\n",
    "negatives = total - positives  # The number of negative samples\n",
    "scale_pos_weight = negatives / (2*total)  # The ratio of negative to positive samples\n",
    "print(f'Using scale_pos_weight = {scale_pos_weight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 1,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'seed': 42,\n",
    "    'verbosity': 1,\n",
    "    'n_jobs': -1,\n",
    "    'eval_metric': 'logloss',\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'gpu_id': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def xgb_model_loader(early_stopping_rounds=2000):\n",
    "    if not os.path.exists('models/checkpoint/xgb_model.model'):\n",
    "        print('Training model...')\n",
    "        xgb_model = xgb.train(params, dtrain, num_boost_round=10000, evals=[(dval, 'val')], early_stopping_rounds=early_stopping_rounds, verbose_eval=100)\n",
    "        xgb_model.save_model('models/checkpoint/xgb_model.model')\n",
    "    else:\n",
    "        print('Loading model...')\n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model('models/checkpoint/xgb_model.model')\n",
    "    assert xgb_model is not None, 'Model not loaded'\n",
    "    return xgb_model\n",
    "\n",
    "def xgb_model_predictions(data_set='test', new_preds=False):\n",
    "    # Load predictions if possible\n",
    "    if os.path.exists(f'data/{data_set}/preds/xgboost/y_pred.npy') and not new_preds:\n",
    "        print('Loading predictions...')\n",
    "        y_pred = np.load(f'data/{data_set}/preds/xgboost/y_pred.npy')\n",
    "        y_pred_proba = np.load(f'data/{data_set}/preds/xgboost/y_pred_proba.npy')\n",
    "        y_true = np.load(f'data/{data_set}/preds/xgboost/y_true.npy')\n",
    "        return y_pred, y_pred_proba, y_true\n",
    "    # Load model\n",
    "    xgb_model = xgb_model_loader()\n",
    "    # Make predictions in the data set\n",
    "    if data_set == 'test':\n",
    "        d = dtest\n",
    "    elif data_set == 'val':\n",
    "        d = dval\n",
    "    elif data_set == 'train':\n",
    "        d = dtrain\n",
    "    else:\n",
    "        raise ValueError('data_set must be one of: train, val, test')\n",
    "    y_pred_proba = xgb_model.predict(d)\n",
    "    y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
    "    y_true = d.get_label()\n",
    "    # Save predictions\n",
    "    PATH_TO_SAVE = f'data/{data_set}/preds/xgboost/'\n",
    "    if not os.path.exists(os.path.dirname(PATH_TO_SAVE)):\n",
    "        os.makedirs(os.path.dirname(PATH_TO_SAVE))\n",
    "    np.save(PATH_TO_SAVE + 'y_pred.npy', y_pred)\n",
    "    np.save(PATH_TO_SAVE + 'y_pred_proba.npy', y_pred_proba)\n",
    "    np.save(PATH_TO_SAVE + 'y_true.npy', y_true)\n",
    "    return y_pred, y_pred_proba, y_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets\n",
    "class MLPDataset(Dataset):\n",
    "    def __init__(self, X, y, device=DEVICE):\n",
    "        self.X = X.to(device)\n",
    "        self.y = y.squeeze().to(device)\n",
    "        self.n = X.shape[0]\n",
    "        # Calculate weights used for BCEWithLogitsLoss\n",
    "        self.zeroes = torch.sum(self.y == 0)\n",
    "        self.ones = torch.sum(self.y == 1)\n",
    "        self.weights = torch.zeros(self.n)\n",
    "        self.weights[self.y == 0] = (self.ones) / (self.n)\n",
    "        self.weights[self.y == 1] = (self.zeroes) / (self.n)\n",
    "        # Normalize weights\n",
    "        self.weights = self.weights / torch.sum(self.weights)\n",
    "        # Move to device\n",
    "        self.weights = self.weights.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.weights[idx]\n",
    "\n",
    "    def get_X(self):\n",
    "        return self.X\n",
    "\n",
    "    def get_y(self):\n",
    "        return self.y\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_X_y_idx(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def get_X_y_weights_idx(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.weights[idx]\n",
    "\n",
    "# Create the dataloaders\n",
    "class MLPDataLoader():\n",
    "    def __init__(self, dataset, batch_size, device=DEVICE, num_workers=num_workers, pin_memory=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.n = len(dataset)\n",
    "        self.m_batches = self.n // self.batch_size\n",
    "        self.indices = np.arange(self.n)\n",
    "        self.i = 0\n",
    "        self.M = self.m_batches * self.batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= self.n:\n",
    "            self.i = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            idx = self.indices[self.i:self.i+self.batch_size]\n",
    "            self.i += self.batch_size\n",
    "            return self.dataset.get_X_y_weights_idx(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.m_batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset.get_X_y_weights_idx(self.indices[idx*self.batch_size:(idx+1)*self.batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MLP = 'models/checkpoint/multilayerPerceptron.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function that is weighted by the class weights\n",
    "def weighted_bce_loss(y_pred, y_true, weights):\n",
    "    return F.binary_cross_entropy_with_logits(y_pred, y_true, weights, reduction='sum')\n",
    "loss_fn = weighted_bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The balanced accuracy metric\n",
    "def balanced_accuracy(y_pred, y_true):\n",
    "    '''Calculates the balanced accuracy'''\n",
    "    y_pred = torch.round(torch.sigmoid(y_pred))\n",
    "    tp = torch.sum(y_pred * y_true)\n",
    "    tn = torch.sum((1 - y_pred) * (1 - y_true))\n",
    "    fp = torch.sum(y_pred * (1 - y_true))\n",
    "    fn = torch.sum((1 - y_pred) * y_true)\n",
    "    tpr = tp / (tp + fn)\n",
    "    tnr = tn / (tn + fp)\n",
    "    return (tpr + tnr) / 2\n",
    "\n",
    "acc = balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size := 128.\n",
      "num_epochs := 256.\n",
      "learning_rate := 1e-05.\n",
      "batch_size := 1048576.\n",
      "weight_decay := 1.0000000000000002e-06.\n",
      "momentum := 0.9.\n",
      "factor := 0.1.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 2**(num_features)\n",
    "num_epochs = 2**(num_features+1)\n",
    "learning_rate = 10**-5\n",
    "batch_size = 2**20\n",
    "weight_decay = learning_rate * 0.1\n",
    "momentum = 0.9\n",
    "factor = 0.1\n",
    "\n",
    "print(f'hidden_size := {hidden_size}.')\n",
    "print(f'num_epochs := {num_epochs}.')\n",
    "print(f'learning_rate := {learning_rate}.')\n",
    "print(f'batch_size := {batch_size}.')\n",
    "print(f'weight_decay := {weight_decay}.')\n",
    "print(f'momentum := {momentum}.')\n",
    "print(f'factor := {factor}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The multi-layer perceptron\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device):\n",
    "        super(MLP, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, device=device)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//2, device=device)\n",
    "        self.fc3 = nn.Linear(hidden_size//2, hidden_size//4, device=device)\n",
    "        self.fc4 = nn.Linear(hidden_size//4, output_size, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "    def predict(self, x):\n",
    "        return torch.sigmoid(self.forward(x))\n",
    "\n",
    "    def predict_class(self, x):\n",
    "        return torch.round(self.predict(x)).squeeze()\n",
    "\n",
    "\n",
    "# The model: multilayerPerceptron\n",
    "multilayerPerceptron = MLP(input_size=num_features, hidden_size=hidden_size, output_size=num_classes, device=DEVICE)\n",
    "\n",
    "# The optimizer\n",
    "optimizer = torch.optim.SGD(multilayerPerceptron.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "# The scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=factor, patience=50, verbose=True)\n",
    "\n",
    "# Define the dataloaders\n",
    "train_dataloader = MLPDataLoader(MLPDataset(X_train, y_train), batch_size, device=DEVICE)\n",
    "val_dataloader = MLPDataLoader(MLPDataset(X_val, y_val), batch_size, device=DEVICE)\n",
    "test_dataloader = MLPDataLoader(MLPDataset(X_test, y_test), batch_size, device=DEVICE)\n",
    "\n",
    "# Train the model\n",
    "def train(model, optimizer, loss_fn, scheduler, train_dataloader, val_dataloader, num_epochs, device):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    best_train_acc = 0\n",
    "    val_accs = []\n",
    "    best_val_acc = 0\n",
    "    best_acc_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for X, y, weights in train_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            weights = weights.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y, weights)\n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc(y_pred, y).item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        train_acc = train_acc / len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        for X, y, weights in val_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            weights = weights.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y, weights)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc(y_pred, y).item()\n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        val_acc = val_acc / len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        # Print\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        # Update the scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        # Save the model if it is the best\n",
    "        if val_acc > best_val_acc and train_acc > 0.5:\n",
    "            best_val_acc = val_acc\n",
    "            best_train_acc = train_acc\n",
    "            best_acc_epoch = epoch\n",
    "            torch.save(model.state_dict(), PATH_TO_MLP)\n",
    "            print(f'Best model saved at epoch {epoch+1}.')\n",
    "    print(f'Best val acc: {best_val_acc:.4f} at epoch {best_acc_epoch+1}.')\n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "#train_losses, val_losses, train_accs, val_accs = train(multilayerPerceptron, optimizer, loss_fn, scheduler, train_dataloader, val_dataloader, num_epochs, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/checkpoint/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "def loadMLP(model=multilayerPerceptron, path=PATH_TO_MLP):\n",
    "    if os.path.isfile(path):\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        print(f'Model loaded from {path}.')\n",
    "    else:\n",
    "        print(f'No model found at {path}.')\n",
    "        train_losses, val_losses, train_accs, val_accs = train(model, optimizer, loss_fn, scheduler, train_dataloader, val_dataloader, num_epochs, DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "multilayerPerceptron = loadMLP(multilayerPerceptron, path='models/checkpoint/model.ckpt')\n",
    "\n",
    "# Function to get (logits, preds, true_labels) pairs\n",
    "def get_preds(model=multilayerPerceptron, dataloader=MLPDataLoader(MLPDataset(X_test, y_test), batch_size, device=DEVICE), data_set='test'):\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y, weights in dataloader:\n",
    "            X = X.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            weights = weights.to(DEVICE)\n",
    "            y_pred = model(X)\n",
    "            logits.append(y_pred)\n",
    "            preds.append(torch.round(y_pred))\n",
    "            true_labels.append(y)\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    true_labels = torch.cat(true_labels, dim=0)\n",
    "    # Save the predictions to files\n",
    "    PATH_STORE_MLP_LOGITS = 'data/{data_set}/preds/mlp/mlp_preds_proba.pt'.format(data_set=dataloader.dataset.data_set)\n",
    "    PATH_STORE_MLP_PREDS = 'data/{data_set}/preds/mlp/mlp_preds.pt'.format(data_set=dataloader.dataset.data_set)\n",
    "    PATH_STORE_MLP_TRUE_LABELS = 'data/{data_set}/preds/mlp/mlp_true.pt'.format(data_set=dataloader.dataset.data_set)\n",
    "    torch.save(logits, PATH_STORE_MLP_LOGITS)\n",
    "    torch.save(preds, PATH_STORE_MLP_PREDS)\n",
    "    torch.save(true_labels, PATH_STORE_MLP_TRUE_LABELS)\n",
    "    return logits, preds, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (relu): ReLU()\n",
      "  (fc1): Linear(in_features=7, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'figs/mlp_architecture.png'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model's architecture\n",
    "print(multilayerPerceptron)\n",
    "# Plot the model's architecture\n",
    "PATH_TO_MLP_ARCHITECTURE = 'figs/mlp_architecture'\n",
    "torchviz.make_dot(multilayerPerceptron(X_train[0:1].to(DEVICE)), params=dict(\n",
    "    multilayerPerceptron.named_parameters()), show_attrs=True, show_saved=True).render(PATH_TO_MLP_ARCHITECTURE, format=\"png\", cleanup=True, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load predictions if new_preds is False\n",
    "def load_mlp_preds(data_set='test', new_preds=False):\n",
    "    if new_preds:\n",
    "        logits, preds, true_labels = get_preds(multilayerPerceptron, MLPDataLoader(MLPDataset(X_test, y_test), batch_size, device=DEVICE, data_set='test'))\n",
    "    else:\n",
    "        PATH_STORE_MLP_LOGITS = 'data/{data_set}/preds/mlp/mlp_preds_proba.pt'.format(data_set=data_set)\n",
    "        PATH_STORE_MLP_PREDS = 'data/{data_set}/preds/mlp/mlp_preds.pt'.format(data_set=data_set)\n",
    "        PATH_STORE_MLP_TRUE_LABELS = 'data/{data_set}/preds/mlp/mlp_true.pt'.format(data_set=data_set)\n",
    "        logits = torch.load(PATH_STORE_MLP_LOGITS)\n",
    "        preds = torch.load(PATH_STORE_MLP_PREDS)\n",
    "        true_labels = torch.load(PATH_STORE_MLP_TRUE_LABELS)\n",
    "    return logits, preds, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model='logreg', new_preds=False):\n",
    "    if model == 'logreg':\n",
    "        # Get the predictions if they already exist\n",
    "        if os.path.exists('data/test/preds/logreg/y_pred.npy') and os.path.exists('data/val/preds/logreg/y_pred.npy') and os.path.exists('data/train/preds/logreg/y_pred.npy') and not new_preds:\n",
    "            print('Loading predictions...')\n",
    "            y_pred_test = np.load('data/test/preds/logreg/y_pred.npy')\n",
    "            y_pred_test_proba = np.load('data/test/preds/logreg/y_pred_proba.npy')\n",
    "            y_test_true = np.load('data/test/preds/logreg/y_true.npy')\n",
    "            y_pred_val = np.load('data/val/preds/logreg/y_pred.npy')\n",
    "            y_pred_val_proba = np.load('data/val/preds/logreg/y_pred_proba.npy')\n",
    "            y_val_true = np.load('data/val/preds/logreg/y_true.npy')\n",
    "            y_pred_train = np.load('data/train/preds/logreg/y_pred.npy')\n",
    "            y_pred_train_proba = np.load('data/train/preds/logreg/y_pred_proba.npy')\n",
    "            y_train_true = np.load('data/train/preds/logreg/y_true.npy')\n",
    "\n",
    "        # Otherwise, fit the model and save the predictions\n",
    "        else:\n",
    "            print('Fitting model...')\n",
    "            logreg = LogisticRegression()\n",
    "            logreg.fit(X_train, y_train)\n",
    "            logreg.save_predictions(X_test, y_test, data_set='test')\n",
    "            logreg.save_predictions(X_val, y_val, data_set='val')\n",
    "            logreg.save_predictions(X_train, y_train, data_set='train')\n",
    "            y_pred_test = logreg.predict(X_test)\n",
    "            y_pred_test_proba = logreg.predict_proba(X_test)\n",
    "            y_pred_test_proba = y_pred_test_proba[:, 1]\n",
    "            y_test_true = y_test\n",
    "            y_pred_val = logreg.predict(X_val)\n",
    "            y_pred_val_proba = logreg.predict_proba(X_val)\n",
    "            y_pred_val_proba = y_pred_val_proba[:, 1]\n",
    "            y_val_true = y_val\n",
    "            y_pred_train = logreg.predict(X_train)\n",
    "            y_pred_train_proba = logreg.predict_proba(X_train)\n",
    "            y_pred_train_proba = y_pred_train_proba[:, 1]\n",
    "            y_train_true = y_train\n",
    "    if model == 'xgboost':\n",
    "        y_pred_test, y_pred_test_proba, y_test_true = xgb_model_predictions(data_set='test', new_preds=new_preds)\n",
    "        y_pred_val, y_pred_val_proba, y_val_true = xgb_model_predictions(data_set='val', new_preds=new_preds)\n",
    "        y_pred_train, y_pred_train_proba, y_train_true = xgb_model_predictions(data_set='train', new_preds=new_preds)\n",
    "    if model == 'mlp':\n",
    "        y_pred_test_proba, y_pred_test, y_test_true = load_mlp_preds(data_set='test', new_preds=new_preds)\n",
    "        y_pred_val_proba, y_pred_val, y_val_true = load_mlp_preds(data_set='val', new_preds=new_preds)\n",
    "        y_pred_train_proba, y_pred_train, y_train_true = load_mlp_preds(data_set='train', new_preds=new_preds)\n",
    "    train_pred, val_pred, test_pred = (y_pred_train, y_pred_train_proba, y_train_true), (y_pred_val, y_pred_val_proba, y_val_true), (y_pred_test, y_pred_test_proba, y_test_true)\n",
    "    return train_pred, val_pred, test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmatrix_plot(model_name='logreg', data_set='test',reload_preds=False):\n",
    "    assert data_set in ['train', 'val', 'test'], 'data_set must be one of \"train\", \"val\", or \"test\"'\n",
    "    index_of_interest = 0 if data_set == 'train' else 1 if data_set == 'val' else 2 if data_set == 'test' else None\n",
    "    y_pred, _, y_true = get_predictions(\n",
    "        model=model_name, new_preds=reload_preds)[index_of_interest]\n",
    "    if torch.is_tensor(y_pred):\n",
    "        y_pred = y_pred.detach().cpu().numpy()\n",
    "    if torch.is_tensor(y_true):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "    cm1 = confusion_matrix(y_true, y_pred, normalize=None, labels=[0, 1])\n",
    "    cm2 = confusion_matrix(y_true, y_pred, normalize='true', labels=[0, 1])\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=300)\n",
    "    sns.heatmap(cm1, annot=True, fmt='d', ax=ax[0], cmap='Blues')\n",
    "    sns.heatmap(cm2, annot=True, fmt='.2f', ax=ax[1], cmap='Blues')\n",
    "    # Model name\n",
    "    if model_name == 'logreg':\n",
    "        model_name = 'Logistic Regression'\n",
    "    elif model_name == 'xgboost':\n",
    "        model_name = 'XGBoost'\n",
    "    elif model_name == 'mlp':\n",
    "        model_name = 'Multilayer Perceptron'\n",
    "    fig.suptitle(model_name, fontsize=16)\n",
    "    # Set titles and labels\n",
    "    ax[0].set_title('Confusion Matrix')\n",
    "    ax[1].set_title('Normalized Confusion Matrix')\n",
    "    ax[0].set_xlabel('Predicted')\n",
    "    ax[0].set_ylabel('True')\n",
    "    ax[1].set_xlabel('Predicted')\n",
    "    ax[1].set_ylabel('True')\n",
    "    # Set ticks\n",
    "    ax[0].set_xticklabels(['Exclude', 'Include'])\n",
    "    ax[0].set_yticklabels(['Exclude', 'Include'])\n",
    "    ax[1].set_xticklabels(['Exclude', 'Include'])\n",
    "    ax[1].set_yticklabels(['Exclude', 'Include'])\n",
    "    plot_name = '{}_{}_cmatrix.png'.format(model_name, data_set)\n",
    "    plt.savefig('figs/{}'.format(plot_name))\n",
    "    plt.show()\n",
    "\n",
    "# Roc curve\n",
    "def roc_plot(model_name='logreg', data_set='test'):\n",
    "    assert data_set in [\n",
    "        'train', 'val', 'test'], 'data_set must be one of \"train\", \"val\", or \"test\"'\n",
    "    if data_set == 'train':\n",
    "        _, y_pred_proba, y_true = get_predictions(model=model_name)[0]\n",
    "    elif data_set == 'val':\n",
    "        _, y_pred_proba, y_true = get_predictions(model=model_name)[1]\n",
    "    elif data_set == 'test':\n",
    "        _, y_pred_proba, y_true = get_predictions(model=model_name)[2]\n",
    "    # Compute the roc curve\n",
    "    if torch.is_tensor(y_pred_proba):\n",
    "        y_pred_proba = y_pred_proba.cpu().detach().numpy()\n",
    "    if torch.is_tensor(y_true):\n",
    "        y_true = y_true.cpu().detach().numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    # Compute the auc\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    # Plot the roc curve\n",
    "    if model_name == 'logreg':\n",
    "        model_name = 'Logistic Regression'\n",
    "    elif model_name == 'xgboost':\n",
    "        model_name = 'XGBoost'\n",
    "    elif model_name == 'mlp':\n",
    "        model_name = 'Multilayer Perceptron'\n",
    "    plt.figure(figsize=(6,3), dpi=300)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.4f})')\n",
    "    plt.plot([np.min(fpr), np.max(fpr)], [np.min(fpr), np.max(fpr)], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plot_name = '{}_{}_roc.png'.format(model_name, data_set)\n",
    "    plt.savefig('figs/{}'.format(plot_name))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the confusion matrix for the test set for the logistic regression model\n",
    "#cmatrix_plot(model_name='mlp', data_set='test', reload_preds=False)\n",
    "\n",
    "# Plot the roc curve for the test set for the logistic regression model\n",
    "#roc_plot(model_name='mlp', data_set='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc curve for all models on test set in one plot and save to file\n",
    "def roc_plot_all_models():\n",
    "    plt.figure(figsize=(6,3), dpi=300)\n",
    "    for model in ['logreg', 'xgboost', 'mlp']:\n",
    "        _, y_pred_proba, y_true = get_predictions(model=model)[2]\n",
    "        # Check if y_pred_proba or y_true is on GPU\n",
    "        try:\n",
    "            if y_pred_proba.is_cuda:\n",
    "                y_pred_proba = y_pred_proba.cpu().detach().numpy()\n",
    "                y_true = y_true.cpu().detach().numpy()\n",
    "        except:\n",
    "            pass\n",
    "        # Compute the roc curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "        # Compute the auc\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        # Plot the roc curve\n",
    "        if model == 'logreg':\n",
    "            model_name = 'Logistic Regression'\n",
    "        elif model == 'xgboost':\n",
    "            model_name = 'XGBoost'\n",
    "        elif model == 'mlp':\n",
    "            model_name = 'Multilayer Perceptron'\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig('figs/all_models_test_roc.png')\n",
    "    plt.show()\n",
    "\n",
    "#roc_plot_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cmatrix for all models on test set in one plot and save to file\n",
    "def cmatrix_plot_all_models():\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(10, 7), dpi=300)\n",
    "    for i, model in enumerate(['logreg', 'xgboost', 'mlp']):\n",
    "        y_pred, _, y_true = get_predictions(model=model)[2]\n",
    "        # Check if y_pred_proba or y_true is on GPU\n",
    "        try:\n",
    "            if y_pred.is_cuda:\n",
    "                y_pred = y_pred.cpu().detach().numpy()\n",
    "                y_true = y_true.cpu().detach().numpy()\n",
    "        except:\n",
    "            pass\n",
    "        cm1 = confusion_matrix(y_true, y_pred, normalize=None, labels=[0, 1])\n",
    "        cm2 = confusion_matrix(y_true, y_pred, normalize='true', labels=[0, 1])\n",
    "        sns.heatmap(cm1, annot=True, fmt='d', ax=ax[0, i], cmap='Blues')\n",
    "        sns.heatmap(cm2, annot=True, fmt='.2f', ax=ax[1, i], cmap='Blues')\n",
    "        # Model name\n",
    "        if model == 'logreg':\n",
    "            model_name = 'Logistic Regression'\n",
    "        elif model == 'xgboost':\n",
    "            model_name = 'XGBoost'\n",
    "        elif model == 'mlp':\n",
    "            model_name = 'Multilayer Perceptron'\n",
    "        # Set titles and labels\n",
    "        # Set the overall title\n",
    "        fig.suptitle('Confusion Matrix', fontsize=16)\n",
    "        ax[0, i].set_title(f'{model_name}')\n",
    "        ax[1, i].set_title(f'{model_name}, Normalized')\n",
    "        ax[0, i].set_xlabel('Predicted')\n",
    "        ax[0, i].set_ylabel('True')\n",
    "        ax[1, i].set_xlabel('Predicted')\n",
    "        ax[1, i].set_ylabel('True')\n",
    "        # Set ticks\n",
    "        ax[0, i].set_xticklabels(['Exclude', 'Include'])\n",
    "        ax[0, i].set_yticklabels(['Exclude', 'Include'])\n",
    "        ax[1, i].set_xticklabels(['Exclude', 'Include'])\n",
    "        ax[1, i].set_yticklabels(['Exclude', 'Include'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figs/all_models_test_cmatrix.png')\n",
    "    plt.show()\n",
    "\n",
    "#cmatrix_plot_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance for the xgboost model\n",
    "def xgboost_feature_importance():\n",
    "    # Load the model\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.load_model('models/checkpoint/xgb_model.model')\n",
    "\n",
    "    # Load labels from lookup_dict.py\n",
    "    labels = lookup_dicts.INDEX_TO_FEATURE\n",
    "    labels = [labels[i] for i in range(len(labels))]\n",
    "    # Get the feature importance\n",
    "    importance = model.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "    # Get the feature importance\n",
    "    importance = model.get_booster().get_score(importance_type='gain')\n",
    "    # Sort the feature importance\n",
    "    importance = {labels[int(k[1:])]: v for k, v in importance.items()}\n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(6,3), dpi=300)\n",
    "    plt.bar(importance.keys(), importance.values())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Feature Importance')\n",
    "    plt.title('Feature Importance for XGBoost Model')\n",
    "    plt.savefig('figs/xgboost_feature_importance.png')\n",
    "    plt.show()\n",
    "\n",
    "#xgboost_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance for the logistic regression model\n",
    "def logreg_feature_importance():\n",
    "    # Load the model using joblib\n",
    "    model = joblib.load('models/checkpoint/logreg.pkl')\n",
    "    # Load labels from lookup_dict.py\n",
    "    labels = lookup_dicts.INDEX_TO_FEATURE\n",
    "    labels = [labels[i] for i in range(len(labels))]\n",
    "    # Get the feature importance\n",
    "    importance = model.coef_[0]\n",
    "    # Sort the feature importance\n",
    "    importance = {labels[i]: importance[i] for i in range(len(importance))}\n",
    "    importance = {k: v for k, v in sorted(importance.items(), key=lambda item: item[1])}\n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(6,3), dpi=300)\n",
    "    plt.bar(importance.keys(), importance.values())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Feature Importance')\n",
    "    plt.title('Feature Importance for Logistic Regression Model')\n",
    "    plt.savefig('figs/logreg_feature_importance.png')\n",
    "    plt.show()\n",
    "\n",
    "#logreg_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions...\n",
      "Loading predictions...\n",
      "Loading predictions...\n",
      "Loading predictions...\n",
      "                   Model    TPR    TNR  Balanced Accuracy     F1    AUC\n",
      "0    Logistic Regression  0.626  0.673              0.650  0.757  0.718\n",
      "1                XGBoost  0.864  0.605              0.735  0.910  0.790\n",
      "2  Multilayer Perceptron  0.996  0.699              0.848  0.985  0.822\n"
     ]
    }
   ],
   "source": [
    "# for all models print the TPR & TNR & Balanced Accuracy  & F1 & AUC in a pandas dataframe and print it\n",
    "\n",
    "def print_metrics():\n",
    "    # Create a dataframe to store the metrics\n",
    "    metrics = pd.DataFrame(columns=['Model', 'TPR', 'TNR', 'Balanced Accuracy', 'F1', 'AUC'])\n",
    "    for i, model in enumerate(['logreg', 'xgboost', 'mlp']):\n",
    "        y_pred, y_pred_proba, y_true = get_predictions(model=model)[2]\n",
    "        # Check if y_pred_proba or y_true is on GPU\n",
    "        try:\n",
    "            if y_pred_proba.is_cuda or y_true.is_cuda or y_pred.is_cuda:\n",
    "                y_pred_proba = y_pred_proba.cpu().detach().numpy()\n",
    "                y_pred = y_pred.cpu().detach().numpy()\n",
    "                y_true = y_true.cpu().detach().numpy()\n",
    "        except:\n",
    "            pass\n",
    "        # Compute the metrics\n",
    "        tpr = recall_score(y_true, y_pred)\n",
    "        tnr = recall_score(y_true, y_pred, pos_label=0)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        # Round the metrics to 3 decimal places\n",
    "        tpr = round(tpr, 3)\n",
    "        tnr = round(tnr, 3)\n",
    "        balanced_accuracy = round(balanced_accuracy, 3)\n",
    "        f1 = round(f1, 3)\n",
    "        auc = round(auc, 3)\n",
    "        # Add the metrics to the dataframe\n",
    "        if model == 'logreg':\n",
    "            model_name = 'Logistic Regression'\n",
    "        elif model == 'xgboost':\n",
    "            model_name = 'XGBoost'\n",
    "        elif model == 'mlp':\n",
    "            model_name = 'Multilayer Perceptron'\n",
    "        metrics.loc[i] = [model_name, tpr, tnr, balanced_accuracy, f1, auc]\n",
    "    # Print the metrics\n",
    "    print(metrics)\n",
    "    return metrics\n",
    "\n",
    "metrics = print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
